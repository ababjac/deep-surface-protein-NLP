{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd0f9ba-23c8-47e0-9904-0c0414d5423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/ababjac/base-venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-03 12:06:41.467673: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-03 12:06:41.703828: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-03 12:06:41.709796: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/isaac/compilers/intel/oneAPI_2021.2.0/mpi/latest/libfabric/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/mpi/latest/lib/release:/sw/isaac/compilers/intel/oneAPI_2021.2.0/mpi/latest/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/ippcp/latest/lib/intel64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/ipp/latest/lib/intel64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/itac/latest/slib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/mkl/latest/lib/intel64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/lib/x64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/lib/emu:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/compiler/lib/intel64_lin:/sw/isaac/compilers/intel/oneAPI_2021.2.0/debugger/latest/dep/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/debugger/latest/libipt/intel64/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/debugger/latest/gdb/intel64/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/tbb/latest/lib/intel64/gcc4.8\n",
      "2023-02-03 12:06:41.709810: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-03 12:06:41.736248: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-03 12:06:42.781630: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/isaac/compilers/intel/oneAPI_2021.2.0/mpi/latest/libfabric/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/mpi/latest/lib/release:/sw/isaac/compilers/intel/oneAPI_2021.2.0/mpi/latest/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/ippcp/latest/lib/intel64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/ipp/latest/lib/intel64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/itac/latest/slib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/mkl/latest/lib/intel64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/lib/x64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/lib/emu:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/compiler/lib/intel64_lin:/sw/isaac/compilers/intel/oneAPI_2021.2.0/debugger/latest/dep/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/debugger/latest/libipt/intel64/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/debugger/latest/gdb/intel64/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/tbb/latest/lib/intel64/gcc4.8\n",
      "2023-02-03 12:06:42.783405: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/isaac/compilers/intel/oneAPI_2021.2.0/mpi/latest/libfabric/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/mpi/latest/lib/release:/sw/isaac/compilers/intel/oneAPI_2021.2.0/mpi/latest/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/ippcp/latest/lib/intel64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/ipp/latest/lib/intel64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/itac/latest/slib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/mkl/latest/lib/intel64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/lib/x64:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/lib/emu:/sw/isaac/compilers/intel/oneAPI_2021.2.0/compiler/latest/linux/compiler/lib/intel64_lin:/sw/isaac/compilers/intel/oneAPI_2021.2.0/debugger/latest/dep/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/debugger/latest/libipt/intel64/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/debugger/latest/gdb/intel64/lib:/sw/isaac/compilers/intel/oneAPI_2021.2.0/tbb/latest/lib/intel64/gcc4.8\n",
      "2023-02-03 12:06:42.783413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6da6bb-64b8-42b3-9785-fd2fd206ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/lustre/isaac/proj/UTK0196/deep-surface-protein-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb7543d-5f95-4a0c-97fc-18ca63929e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH+'M0059E_training_set.tsv', delimiter=',', nrows=5000, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc7819bd-6b61-4caa-a5e6-c909363b829c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTERGKYVCVGLGEILWDMLPEGKQLGGAPANFAYHAQALRGQGVV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MTVDGKYLCVGLGEILWDMLPGGKQLGGAPANFAYHSQALGAQGVV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MKVALVGLLQSGKSTILASLSGKAIPAIGSASIEEAIVSVPDDRFD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MKVALIGLLQSGKSTILASLTGKAIPAIGSASIEETIVPVPDERFD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MKVALIGLLQSGKSTILASLTGKAVPAAGSASIEEAIVPVPDERFD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>MDKDTVILVVDDEREHADGIAEAMEKLCGRAIAVYNGADALEIVRN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>MDKDTVILVVDDEREHADGIAEAMEKLCGRAIAVYNGADALEIVRN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>MDKDTVILVVDDEREHADGIAEAMEKLCGRAIAVYNGADALEIVRN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>MDKDTVILVVDDEREHADGIAEAMEKLCGRAIAVYNGADALEIVRN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>MDKDTVILVVDDEREHADGIAEAMEKLCGRAIAVYNGADALEIVRN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     MTERGKYVCVGLGEILWDMLPEGKQLGGAPANFAYHAQALRGQGVV...      0\n",
       "1     MTVDGKYLCVGLGEILWDMLPGGKQLGGAPANFAYHSQALGAQGVV...      0\n",
       "2     MKVALVGLLQSGKSTILASLSGKAIPAIGSASIEEAIVSVPDDRFD...      0\n",
       "3     MKVALIGLLQSGKSTILASLTGKAIPAIGSASIEETIVPVPDERFD...      0\n",
       "4     MKVALIGLLQSGKSTILASLTGKAVPAAGSASIEEAIVPVPDERFD...      0\n",
       "...                                                 ...    ...\n",
       "9995  MDKDTVILVVDDEREHADGIAEAMEKLCGRAIAVYNGADALEIVRN...      1\n",
       "9996  MDKDTVILVVDDEREHADGIAEAMEKLCGRAIAVYNGADALEIVRN...      1\n",
       "9997  MDKDTVILVVDDEREHADGIAEAMEKLCGRAIAVYNGADALEIVRN...      1\n",
       "9998  MDKDTVILVVDDEREHADGIAEAMEKLCGRAIAVYNGADALEIVRN...      1\n",
       "9999  MDKDTVILVVDDEREHADGIAEAMEKLCGRAIAVYNGADALEIVRN...      1\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surf_series = df['surf.sequence']\n",
    "deep_series = df['deep.sequence']\n",
    "\n",
    "classification_df = pd.DataFrame({'text' : surf_series.append(deep_series, ignore_index=True), 'label' : [0]*surf_series.size+[1]*deep_series.size})\n",
    "classification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1896f665-6e69-491f-a842-4e469f092f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_sequence(seq, word_length, overlap):\n",
    "    if overlap >= word_length:\n",
    "        print('Overlap must be less than word length')\n",
    "        return\n",
    "    \n",
    "    for i in range(0, len(seq)-overlap, word_length-overlap):\n",
    "        yield seq[i:i+word_length]\n",
    "        \n",
    "def get_overlap_array(seq, word_length=5, overlap=2):\n",
    "    return np.array(list(overlap_sequence(seq, word_length, overlap)))\n",
    "\n",
    "def get_overlap_string(seq, word_length=5, overlap=2):\n",
    "    return ' '.join(list(overlap_sequence(seq, word_length, overlap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8857b292-49c3-4d87-95e8-4c2edde2ea88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTERG RGKYV YVCVG VGLGE GEILW LWDML MLPEG EGKQ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MTVDG DGKYL YLCVG VGLGE GEILW LWDML MLPGG GGKQ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MKVAL ALVGL GLLQS QSGKS KSTIL ILASL SLSGK GKAI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MKVAL ALIGL GLLQS QSGKS KSTIL ILASL SLTGK GKAI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MKVAL ALIGL GLLQS QSGKS KSTIL ILASL SLTGK GKAV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>MDKDT DTVIL ILVVD VDDER EREHA HADGI GIAEA EAME...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>MDKDT DTVIL ILVVD VDDER EREHA HADGI GIAEA EAME...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>MDKDT DTVIL ILVVD VDDER EREHA HADGI GIAEA EAME...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>MDKDT DTVIL ILVVD VDDER EREHA HADGI GIAEA EAME...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>MDKDT DTVIL ILVVD VDDER EREHA HADGI GIAEA EAME...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     MTERG RGKYV YVCVG VGLGE GEILW LWDML MLPEG EGKQ...      0\n",
       "1     MTVDG DGKYL YLCVG VGLGE GEILW LWDML MLPGG GGKQ...      0\n",
       "2     MKVAL ALVGL GLLQS QSGKS KSTIL ILASL SLSGK GKAI...      0\n",
       "3     MKVAL ALIGL GLLQS QSGKS KSTIL ILASL SLTGK GKAI...      0\n",
       "4     MKVAL ALIGL GLLQS QSGKS KSTIL ILASL SLTGK GKAV...      0\n",
       "...                                                 ...    ...\n",
       "9995  MDKDT DTVIL ILVVD VDDER EREHA HADGI GIAEA EAME...      1\n",
       "9996  MDKDT DTVIL ILVVD VDDER EREHA HADGI GIAEA EAME...      1\n",
       "9997  MDKDT DTVIL ILVVD VDDER EREHA HADGI GIAEA EAME...      1\n",
       "9998  MDKDT DTVIL ILVVD VDDER EREHA HADGI GIAEA EAME...      1\n",
       "9999  MDKDT DTVIL ILVVD VDDER EREHA HADGI GIAEA EAME...      1\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_df['text'] = classification_df['text'].transform(get_overlap_string)\n",
    "classification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba10aa6-6a07-41b0-9170-0f6f9fa702e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(classification_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e4dd52-3da0-46fc-887e-4fba4a27b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdaa9fdc-6daf-4eab-96e8-a836906241ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ad1e5fe-6b63-4255-a27a-307ebd0280cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:11<00:00,  1.16s/ba]\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = ds.map(lambda d : tokenizer(d['text'], return_tensors=\"pt\", padding=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27cad4f5-53e4-416a-92a6-6217b86aff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_splits = tokenized_ds.train_test_split(test_size=0.2)\n",
    "\n",
    "tmp = init_splits['train']\n",
    "test_ds = init_splits['test']\n",
    "\n",
    "splits = tmp.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de8e6e40-5b3f-4c53-a393-34ab06fe626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'esm.contact_head.regression.bias', 'esm.contact_head.regression.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = EsmForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "#logits = model(**inputs).logits\n",
    "#predicted_class_id = logits.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dad9130-f0cc-4753-90cd-e0b641b5b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./train_output2',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    #data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f0d179a-f627-4c00-ae6b-bd3b7efc00e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `EsmForSequenceClassification.forward` and have been ignored: text. If text are not expected by `EsmForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/nfs/home/ababjac/base-venv/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 7200\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 33:43:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.647300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.499200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.368400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_output2/checkpoint-500\n",
      "Configuration saved in ./train_output2/checkpoint-500/config.json\n",
      "Model weights saved in ./train_output2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./train_output2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./train_output2/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./train_output2/checkpoint-1000\n",
      "Configuration saved in ./train_output2/checkpoint-1000/config.json\n",
      "Model weights saved in ./train_output2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./train_output2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./train_output2/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./train_output2/checkpoint-1500\n",
      "Configuration saved in ./train_output2/checkpoint-1500/config.json\n",
      "Model weights saved in ./train_output2/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./train_output2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./train_output2/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./train_output2/checkpoint-2000\n",
      "Configuration saved in ./train_output2/checkpoint-2000/config.json\n",
      "Model weights saved in ./train_output2/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./train_output2/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./train_output2/checkpoint-2000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=0.5262776217990451, metrics={'train_runtime': 121487.6953, 'train_samples_per_second': 0.296, 'train_steps_per_second': 0.019, 'total_flos': 4969130420173248.0, 'train_loss': 0.5262776217990451, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b086e62-4459-4541-a58e-f0fb05657a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `EsmForSequenceClassification.forward` and have been ignored: text. If text are not expected by `EsmForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 800\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5929588079452515,\n",
       " 'eval_runtime': 640.5326,\n",
       " 'eval_samples_per_second': 1.249,\n",
       " 'eval_steps_per_second': 0.078,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd233bcc-bea7-4281-83d7-9596043fa4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `EsmForSequenceClassification.forward` and have been ignored: text. If text are not expected by `EsmForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-1.0249838 ,  1.0563256 ],\n",
       "       [ 1.0486145 , -1.1835587 ],\n",
       "       [ 0.889717  , -1.025582  ],\n",
       "       ...,\n",
       "       [ 0.19256382, -0.42237854],\n",
       "       [-0.47040325,  0.6650371 ],\n",
       "       [ 1.0476136 , -1.0625465 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 1, 1, 0]), metrics={'test_loss': 0.6171581745147705, 'test_runtime': 1716.1217, 'test_samples_per_second': 1.165, 'test_steps_per_second': 0.073})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_dataset=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b933119-945b-4675-a9bc-9fdf835b92ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3555545/1363040304.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/initial-esm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "trainer.save_pretrained('./models/initial-esm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
